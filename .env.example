# Palma.py Configuration

# General Configuration

# Debug mode
# False for production enviroments
DEBUG=True

# We define threads max workers shared Thread Pool Executor
# This allows you to do multiple inference at the same time
# Any additional requests over the limit will be queued
THREADS_MAX_WORKERS=10

# Model Settings

## Insert the hugging face model id
MODEL_ID=meta-llama/Meta-Llama-3-8B-Instruct

## Lets set the terminators converter
CONVERT_TOKENS_TO_IDS=<|eot_id|>

## Set the Pytorch float type
TORCH_DTYPE=bfloat16

# Model Inference Settings

## Set the default value for do sample
DEFAULT_DO_SAMPLE=True

## Set the default max tokens to generate
DEFAULT_MAX_NEW_TOKENS=256

## Set the default model temperature
DEFAULT_TEMPERATURE=0.6

## Set the default model top_p
DEFAULT_TOP_P=0.7

# CORS Values

## Set the cache time for pre-flight requests
DEFAULT_ACCESS_CONTROL_MAX_AGE=86400

## Comma separated values for the domains you want to allow
## Or "*" for all
DEFAULT_ACCESS_CONTROL_ALLOW_ORIGIN=*